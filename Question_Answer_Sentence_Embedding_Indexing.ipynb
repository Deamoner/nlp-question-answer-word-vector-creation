{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question/Answer Sentence Embedding Indexing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2GLlw0tvxjN",
        "colab_type": "text"
      },
      "source": [
        "# NLP Clinical Trials - Creating a Question/Answer Machine Simular to Westworld Robheum\n",
        "\n",
        "What really needs to be done:\n",
        "\n",
        "1.   Load the data - done \n",
        "2.   Create the vectors file - on the way \n",
        "3.   Utilize the vectorization for creationg sentence embedding index for impliarity  \n",
        "3.   Process sentence embedding and index\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHDnVj7IUtaV",
        "colab_type": "code",
        "outputId": "62e5ef91-65c2-41ea-c216-fb2050936e85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "!pip install fasttext"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting fasttext\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/85/e2b368ab6d3528827b147fdb814f8189acc981a4bc2f99ab894650e05c40/fasttext-0.9.2.tar.gz (68kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 20kB 1.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 2.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext) (2.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext) (46.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext) (1.18.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp36-cp36m-linux_x86_64.whl size=3019051 sha256=088ebb9b60434f6266b9e9147150f5fbd807abce222555f30b5bab3fbfea230e\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/ba/7f/b154944a1cf5a8cee91c154b75231136cc3a3321ab0e30f592\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWOLNOELyh3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading Section \n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import sqlite3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92VUSi1sxd0z",
        "colab_type": "text"
      },
      "source": [
        "#Load Data Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbZmCxMtvoTE",
        "colab_type": "code",
        "outputId": "daed09a0-e5d7-43f8-cf34-60b8026a510d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#Load the data through google drive\n",
        "#Section for Google Drive OAuth\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uplc_5ZmxGls",
        "colab_type": "code",
        "outputId": "ddcc4481-c30a-422c-e1d6-6b78ce9e67d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "try:\n",
        "    os.stat(\"./data/\")\n",
        "except:\n",
        "    os.mkdir(\"./data/\")  \n",
        "shutil.copy(\"../content/gdrive/My Drive/articles.sqlite\", \"data\")\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data/articles.sqlite'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYPxiWUFyVI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load databbase into memory\n",
        "# Connect to database\n",
        "db = sqlite3.connect(\"data/articles.sqlite\")\n",
        "pd.set_option(\"max_colwidth\", 125)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9OeEWeuxgGM",
        "colab_type": "text"
      },
      "source": [
        "#Analyze Data Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5vljpNBxamb",
        "colab_type": "code",
        "outputId": "cc4aaa82-5590-4f7d-e282-6b153269fc92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "articles = pd.read_sql_query(\"select Id,Title,Tags,Size,Method from articles where Size > 0 LIMIT 5\", db)\n",
        "articles.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Title</th>\n",
              "      <th>Tags</th>\n",
              "      <th>Size</th>\n",
              "      <th>Method</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8e9605de5913710c88b1107fd65dcc26ab372805</td>\n",
              "      <td>Comparison of respiratory pathogen yields from Nasopharyngeal/Oropharyngeal swabs and sputum specimens collected from hos...</td>\n",
              "      <td>COVID-19</td>\n",
              "      <td>2019</td>\n",
              "      <td>We compared pathogen yields from NP/OP swabs and sputum specimens from patients ≥18 years hospitalized with ALRI in rural...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>d958168df85240e544a918d843a14e887dc41d2b</td>\n",
              "      <td>Note from the editors: novel coronavirus (2019-nCoV)</td>\n",
              "      <td>COVID-19</td>\n",
              "      <td>20</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4b36607cdbc54f8006161a9a1839489dd0a51269</td>\n",
              "      <td>A qualitative study of zoonotic risk factors among rural communities in southern China</td>\n",
              "      <td>COVID-19</td>\n",
              "      <td>88</td>\n",
              "      <td>METHODS: Residents in rural communities of Yunnan, Guangxi and Guangdong provinces were recruited and enrolled in this st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>d99dbae98cc9705d9b5674bb6eb66560b4434305</td>\n",
              "      <td>The third coronavirus epidemic in the third millennium: what’s next?</td>\n",
              "      <td>COVID-19</td>\n",
              "      <td>2500</td>\n",
              "      <td>THe eMeRGence oF covId-19 cAused by sARs-cov-2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>53a3bfcb863542f8e6422ff2ae753f204fddf83d</td>\n",
              "      <td>The impact of point-of-care testing for influenza A and B on patient flow and management in a medical assessment unit of ...</td>\n",
              "      <td>COVID-19</td>\n",
              "      <td>54</td>\n",
              "      <td>Data was gathered from the Hospital LIS and Patient Information Management System (PIMS, iSoft) which were retrospectivel...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         Id  ...                                                                                                                        Method\n",
              "0  8e9605de5913710c88b1107fd65dcc26ab372805  ...  We compared pathogen yields from NP/OP swabs and sputum specimens from patients ≥18 years hospitalized with ALRI in rural...\n",
              "1  d958168df85240e544a918d843a14e887dc41d2b  ...                                                                                                                          None\n",
              "2  4b36607cdbc54f8006161a9a1839489dd0a51269  ...  METHODS: Residents in rural communities of Yunnan, Guangxi and Guangdong provinces were recruited and enrolled in this st...\n",
              "3  d99dbae98cc9705d9b5674bb6eb66560b4434305  ...                                                                                THe eMeRGence oF covId-19 cAused by sARs-cov-2\n",
              "4  53a3bfcb863542f8e6422ff2ae753f204fddf83d  ...  Data was gathered from the Hospital LIS and Patient Information Management System (PIMS, iSoft) which were retrospectivel...\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWvgfutp2nlJ",
        "colab_type": "text"
      },
      "source": [
        "#Build the embeddings \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wcokhcD4gzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Define some tokenizer objects:\n",
        "\"\"\"\n",
        "Text tokenization methods\n",
        "\"\"\"\n",
        "\n",
        "import re\n",
        "import string\n",
        "\n",
        "class Tokenizer(object):\n",
        "    \"\"\"\n",
        "    Text tokenization methods\n",
        "    \"\"\"\n",
        "\n",
        "    # English Stop Word List (Standard stop words used by Apache Lucene)\n",
        "    STOP_WORDS = {\"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"if\", \"in\", \"into\", \"is\", \"it\",\n",
        "                  \"no\", \"not\", \"of\", \"on\", \"or\", \"such\", \"that\", \"the\", \"their\", \"then\", \"there\", \"these\",\n",
        "                  \"they\", \"this\", \"to\", \"was\", \"will\", \"with\"}\n",
        "\n",
        "  \n",
        "    def tokenize(text):\n",
        "        \"\"\"\n",
        "        Tokenizes input text into a list of tokens. Filters tokens that match a specific pattern and removes stop words.\n",
        "        Args:\n",
        "            text: input text\n",
        "        Returns:\n",
        "            list of tokens\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert to all lowercase, split on whitespace, strip punctuation\n",
        "        tokens = [token.strip(string.punctuation) for token in text.lower().split()]\n",
        "\n",
        "        # Tokenize on alphanumeric strings.\n",
        "        # Require strings to be at least 2 characters long.\n",
        "        # Require at least 1 alpha character in string.\n",
        "        return [token for token in tokens if re.match(r\"^\\d*[a-z][\\-.0-9:_a-z]{1,}$\", token) and token not in Tokenizer.STOP_WORDS]\n",
        "\n",
        "\n",
        "\n",
        "class RowIterator(object):\n",
        "    \"\"\"\n",
        "    Iterates over rows in a database query. Allows for multiple iterations.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dbfile):\n",
        "        \"\"\"\n",
        "        Initializes RowIterator.\n",
        "        Args:\n",
        "            dbfile: path to SQLite file\n",
        "        \"\"\"\n",
        "\n",
        "        # Store database file\n",
        "        self.dbfile = dbfile\n",
        "\n",
        "        self.rows = self.stream(self.dbfile)\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "        Creates a database query generator.\n",
        "        Returns:\n",
        "            generator\n",
        "        \"\"\"\n",
        "\n",
        "        # reset the generator\n",
        "        self.rows = self.stream(self.dbfile)\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        \"\"\"\n",
        "        Gets the next result in the current generator.\n",
        "        Returns:\n",
        "            tokens\n",
        "        \"\"\"\n",
        "\n",
        "        result = next(self.rows)\n",
        "        if result is None:\n",
        "            raise StopIteration\n",
        "        else:\n",
        "            return result\n",
        "\n",
        "    def stream(self, dbfile):\n",
        "        \"\"\"\n",
        "        Connects to SQLite file at dbfile and yields parsed tokens for each row.\n",
        "        Args:\n",
        "            dbfile:\n",
        "        \"\"\"\n",
        "\n",
        "        # Connection to database file\n",
        "        db = sqlite3.connect(dbfile)\n",
        "        cur = db.cursor()\n",
        "\n",
        "        cur.execute(\"SELECT Text FROM sections\")\n",
        "\n",
        "        count = 0\n",
        "        for section in cur:\n",
        "            # Tokenize text\n",
        "            tokens = Tokenizer.tokenize(section[0])\n",
        "\n",
        "            count += 1\n",
        "            if count % 1000 == 0:\n",
        "                print(\"Streamed %d documents\" % (count), end=\"\\r\")\n",
        "\n",
        "            # Skip documents with no tokens parsed\n",
        "            if tokens:\n",
        "                yield tokens\n",
        "\n",
        "        print(\"Iterated over %d total rows\" % (count))\n",
        "\n",
        "        # Free database resources\n",
        "        db.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHubMTuy3gjE",
        "colab_type": "code",
        "outputId": "eaa260f0-aa21-4af7-d6ea-124d6c901f19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#First let's get all the text from the db\n",
        "import tempfile\n",
        "# Connection to database file\n",
        "cur = db.cursor()\n",
        "\n",
        "cur.execute(\"SELECT Text FROM sections\")\n",
        "\n",
        "count = 0\n",
        "#print(cur)\n",
        "dbfile = \"data/articles.sqlite\"\n",
        "\n",
        "#loop to write tokens to a temporary file \n",
        "with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".txt\", delete=False) as output:\n",
        "          # Save file path\n",
        "          tokens = output.name\n",
        "\n",
        "          for row in RowIterator(dbfile):\n",
        "              output.write(\" \".join(row) + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #this is the text what needs to be done: \n",
        "    #1st is to tokenize all the text - write tokens to a temporary file \n",
        "    #turn tokenze into vectors - we may want to stream the tokens to a temporary file \n",
        "    #create model using the vectors \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iterated over 11390230 total rows\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR5LB746VN7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training the vectors \n",
        "# Train on tokens file using largest dimension size\n",
        "\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "size = 300 \n",
        "mincount = 3\n",
        "model = fasttext.train_unsupervised(tokens, dim=size, minCount=mincount)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuiAmAhzWvEO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Next is to take the vectors in the model and writing it out to a file and converting \n",
        "# Output file path\n",
        "\n",
        "path = \"data/covid-fasttext-vectors\"\n",
        "\n",
        "# Output vectors in vec/txt format\n",
        "with open(path + \".txt\", \"w\") as output:\n",
        "    words = model.get_words()\n",
        "    output.write(\"%d %d\\n\" % (len(words), model.get_dimension()))\n",
        "\n",
        "    for word in words:\n",
        "        # Skip end of line token\n",
        "        if word != \"</s>\":\n",
        "            vector = model.get_word_vector(word)\n",
        "            data = \"\"\n",
        "            for v in vector:\n",
        "                data += \" \" + str(v)\n",
        "\n",
        "            output.write(word + data + \"\\n\")\n",
        "\n",
        "# Build magnitude vectors database\n",
        "print(\"Converting vectors to magnitude format\")\n",
        "converter.convert(path + \".txt\", path + \".magnitude\", subword=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rykXEKi6Kg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for section in cur:\n",
        "      # Tokenize text\n",
        "      tokens = Tokenizer.tokenize(section[0])\n",
        "\n",
        "      count += 1\n",
        "      if count % 1000 == 0:\n",
        "          print(\"Streamed %d documents\" % (count), end=\"\\r\")\n",
        "\n",
        "      # Skip documents with no tokens parsed\n",
        "      if tokens:\n",
        "          yield tokens\n",
        "print(\"Iterated over %d total rows\" % (count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRZR_eJO5rum",
        "colab_type": "code",
        "outputId": "888386f5-ed78-4e67-ba88-2b39d9e6915a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "\n",
        "  \n",
        "import zipfile\n",
        "#Load subsequent required files \n",
        "#Fast Vectors Load - Precomputed Word Vectors the document \n",
        "# https://www.kaggle.com/davidmezzetti/cord19-fasttext-vectors\n",
        "#shutil.copy(\"../content/gdrive/My Drive/cord19-fasttext-vectors.zip\", \"data\")\n",
        "with zipfile.ZipFile('./data/cord19-fasttext-vectors.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('./data/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement zipfile (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for zipfile\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FX8UpLE2c7t",
        "colab_type": "code",
        "outputId": "77d8bc6c-7d90-45b0-be0f-9705770eae30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#The sections related to all the articles and all the sentences\n",
        "sections = pd.read_sql_query(\"select * from sections LIMIT 5\", db)\n",
        "#sections.head()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sequence requirements for RNA strand transfer during nidovirus discontinuous subgenomic RNA synthesis\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5dzmG9nzT8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Fast Vectors Load - Precomputed Word Vectors the document \n",
        "# https://www.kaggle.com/davidmezzetti/cord19-fasttext-vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5X9NCyyzqTG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Compute code\n",
        "!pip install git+https://github.com/neuml/cord19q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR7AySu71sDh",
        "colab_type": "text"
      },
      "source": [
        "#Using the clinical study attribute models created "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVYnzjSs1oDu",
        "colab_type": "code",
        "outputId": "69f2a3e8-179e-4734-8ac0-b506cfd04e6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Not really needed but nice to see how easy it is to move over the clinical study attributes models\n",
        "import pickle\n",
        "load_model = pickle.load(open('abc.sav','rb'))\n",
        "sections = pd.read_sql_query(\"select * from sections LIMIT 5\", db)\n",
        "#sections.head()\n",
        "tempsentence = sections.iloc[0][\"Text\"]\n",
        "#Inference Demo\n",
        "#One Sentence Test of Prediction\n",
        "tempsentence = pd.Series(tempsentence)\n",
        "#Run Predictions \n",
        "#prediction_all= load_model.predict(testsentences)\n",
        "predict_one = load_model.predict(tempsentence)\n",
        "print(predict_one)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pACm0PE7jj8",
        "colab_type": "text"
      },
      "source": [
        "#Building the embedding Index\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQXhUIuJ7l1M",
        "colab_type": "code",
        "outputId": "9bfcc2f6-72b7-4a97-9316-caa019386474",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "source": [
        "import shutil\n",
        "\n",
        "from cord19q.index import Index\n",
        "\n",
        "# Copy vectors locally for predictable performance\n",
        "shutil.copy(\"./data/cord19-300d.magnitude\", \"./tmp\")\n",
        "\n",
        "# Build the embeddings index\n",
        "Index.run(\"./data/articles.sqlite\", \"./tmp/cord19-300d.magnitude\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-46826a81db15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcord19q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Copy vectors locally for predictable performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cord19q'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}